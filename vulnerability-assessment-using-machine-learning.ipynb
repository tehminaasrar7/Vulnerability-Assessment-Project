{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8469919,"sourceType":"datasetVersion","datasetId":5050286}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Step 1: Load and Preprocess the Data\n\nIn this step, we load the CSV file containing the vulnerability assessment data. We ensure that the date columns are properly formatted as datetime objects. Additionally, we create new features such as 'Description Length' which measures the length of the description text, and 'Days to Patch' which calculates the number of days between the discovery date and the patch date. We also handle any infinite values by replacing them with NaN and dropping any rows that contain NaN values to clean the data.\n","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\n\n# Load the CSV file\nfile_path = '/kaggle/input/vulnerability-assessment-data-csv/vulnerability_assessment_data.csv'\ndf = pd.read_csv(file_path)\n\n# Ensure date columns are in datetime format\ndf['Discovered Date'] = pd.to_datetime(df['Discovered Date'])\ndf['Patch Date'] = pd.to_datetime(df['Patch Date'])\n\n# Create additional features\ndf['Description Length'] = df['Description'].apply(len)\ndf['Days to Patch'] = (df['Patch Date'] - df['Discovered Date']).dt.days\n\n# Replace infinite values with NaN and drop rows with NaN values\ndf.replace([float('inf'), -float('inf')], pd.NA, inplace=True)\ndf.dropna(inplace=True)\n\n# Display the first few rows of the DataFrame to ensure it's loaded correctly\ndf.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T05:57:33.218552Z","iopub.execute_input":"2024-05-21T05:57:33.218991Z","iopub.status.idle":"2024-05-21T05:57:33.261683Z","shell.execute_reply.started":"2024-05-21T05:57:33.218957Z","shell.execute_reply":"2024-05-21T05:57:33.260412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Distribution of Risk Scores\nHere, we visualize the distribution of risk scores in the dataset. This helps us understand the range and frequency of different risk scores. A histogram with a kernel density estimate (KDE) is used to plot this distribution, giving us insights into how risk scores are spread across the dataset.\n","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries for plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Distribution of risk scores\nplt.figure(figsize=(10, 6))\nsns.histplot(df['Risk Score'], bins=10, kde=True)\nplt.title('Distribution of Risk Scores')\nplt.xlabel('Risk Score')\nplt.ylabel('Frequency')\nplt.savefig('distribution_of_risk_scores.png')  # Save the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T05:58:27.083503Z","iopub.execute_input":"2024-05-21T05:58:27.084020Z","iopub.status.idle":"2024-05-21T05:58:27.691135Z","shell.execute_reply.started":"2024-05-21T05:58:27.083979Z","shell.execute_reply":"2024-05-21T05:58:27.689888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3: Relationship Between Days to Patch and Severity\nThis step involves analyzing the relationship between the severity of vulnerabilities and the time taken to patch them. A boxplot is used to visualize this relationship, which helps in identifying any patterns or trends in the data, such as whether more severe vulnerabilities are patched faster or slower than less severe ones.\n","metadata":{}},{"cell_type":"markdown","source":"# Step 4: One-Hot Encoding of Categorical Variables\nWe convert categorical variables such as 'Severity', 'Vulnerability Type', and 'Affected Software' into a format that can be used for model training. This process is called one-hot encoding, where each category is represented as a binary vector. We then define our features (X) and target variable (y), and split the dataset into training and testing sets.\n\n","metadata":{}},{"cell_type":"code","source":"# One-hot encode categorical variables\ncategorical_cols = ['Severity', 'Vulnerability Type', 'Affected Software']\ndf_encoded = pd.get_dummies(df, columns=categorical_cols)\n\n# Define the features and target variable\nX = df_encoded.drop(columns=['ID', 'Description', 'Discovered Date', 'Patch Date', 'Risk Score'])\ny = df_encoded['Risk Score']\n\n# Split the data into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Display the first few rows of the training data to ensure it's processed correctly\nX_train.head()\ny_train.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:00:29.532081Z","iopub.execute_input":"2024-05-21T06:00:29.532537Z","iopub.status.idle":"2024-05-21T06:00:29.555667Z","shell.execute_reply.started":"2024-05-21T06:00:29.532501Z","shell.execute_reply":"2024-05-21T06:00:29.554140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 5: Model Training\nIn this step, we train a machine learning model using the Random Forest algorithm. The model is trained on the training data to learn the patterns and relationships between the features and the target variable (risk score). The Random Forest classifier is chosen for its robustness and ability to handle a variety of data types.\n","metadata":{}},{"cell_type":"code","source":"# Initialize and train the model\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:01:26.905603Z","iopub.execute_input":"2024-05-21T06:01:26.906009Z","iopub.status.idle":"2024-05-21T06:01:27.170155Z","shell.execute_reply.started":"2024-05-21T06:01:26.905979Z","shell.execute_reply":"2024-05-21T06:01:27.168860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 6: Model Evaluation\nAfter training the model, we evaluate its performance using the test data. We make predictions on the test set and calculate the accuracy of the model. Additionally, we generate a classification report that provides detailed metrics such as precision, recall, and F1-score for each class. We also visualize the performance of the model using a confusion matrix, which shows the number of correct and incorrect predictions for each class.\n","metadata":{}},{"cell_type":"code","source":"# Make predictions\nfrom sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n\ny_pred = model.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy}\")\n\nclassification_rep = classification_report(y_test, y_pred)\nprint(f\"Classification Report:\\n{classification_rep}\")\n\n# Plot the confusion matrix\ndisp = ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, cmap='Blues')\nplt.title('Confusion Matrix')\nplt.savefig('confusion_matrix.png')  # Save the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:04:33.121369Z","iopub.execute_input":"2024-05-21T06:04:33.121841Z","iopub.status.idle":"2024-05-21T06:04:34.088320Z","shell.execute_reply.started":"2024-05-21T06:04:33.121802Z","shell.execute_reply":"2024-05-21T06:04:34.087051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 7: Feature Importances\nUnderstanding which features are most important for the model's predictions is crucial. In this step, we extract and visualize the feature importances from the trained Random Forest model. This helps us identify which factors contribute most to the risk scores, providing insights into the key drivers of vulnerability risk in the dataset.\n","metadata":{}},{"cell_type":"code","source":"# Get feature importances\nfeature_importances = pd.DataFrame({'Feature': X_train.columns, 'Importance': model.feature_importances_})\nfeature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n\n# Display the feature importances\nfeature_importances\n\n# Plot the feature importances\nplt.figure(figsize=(12, 8))\nsns.barplot(x='Importance', y='Feature', data=feature_importances)\nplt.title('Feature Importances')\nplt.savefig('feature_importances.png')  # Save the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:05:50.795177Z","iopub.execute_input":"2024-05-21T06:05:50.795886Z","iopub.status.idle":"2024-05-21T06:05:51.432121Z","shell.execute_reply.started":"2024-05-21T06:05:50.795850Z","shell.execute_reply":"2024-05-21T06:05:51.430765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final Summary\n\nIn this project, we conducted a comprehensive vulnerability assessment using machine learning techniques. We started by loading and preprocessing the data, ensuring it was clean and well-formatted. We then explored the distribution of risk scores and analyzed the relationship between the time taken to patch vulnerabilities and their severity.\n\nUsing one-hot encoding, we prepared the data for machine learning and trained a Random Forest classifier to predict risk scores. The model's performance was evaluated, and we achieved a good level of accuracy. Finally, we identified the most important features influencing the risk scores, which can help inform future vulnerability management strategies.\n\nOverall, this project demonstrates how data science and machine learning can be applied to enhance vulnerability assessment processes, providing valuable insights and aiding in the effective management of security risks.\n","metadata":{}}]}